{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        \n",
    "    def is_number(self):\n",
    "        if re.search(r'^\\d+$', self.val):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def is_contain_digit (self):\n",
    "        if re.search(r'\\d', self.val):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_date(self):\n",
    "        try: \n",
    "            parse(self.val, fuzzy=True)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    ls = []\n",
    "    sentence = re.sub(r'[.,]', ' ', sentence)\n",
    "    for word in re.split(r' +', sentence):\n",
    "        if word != '':\n",
    "            ls.append(Token(word))\n",
    "            \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(p):\n",
    "    ls = []\n",
    "    for s in re.split(r'\\n+',p):\n",
    "        if not re.match(r' +?$', s):\n",
    "            ls.append(s.strip())\n",
    "        \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(sentences):\n",
    "    ls = []\n",
    "    \n",
    "    sentences = tokenize(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        word = sentences[i].val\n",
    "        ls.append(f'{word}')\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(sentences):\n",
    "    ls = []\n",
    "    \n",
    "    sentences = f'<s> {sentences} </s>'\n",
    "    sentences = tokenize(sentences)\n",
    "    for i in range(len(sentences) - 1):\n",
    "        word = sentences[i].val\n",
    "        word_i = sentences[i + 1].val\n",
    "        ls.append(f'{word}_{word_i}')\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram(sentences):\n",
    "    ls = []\n",
    "    \n",
    "    sentences = f'<s> <s> {sentences} </s> </s>'\n",
    "    sentences = tokenize(sentences)\n",
    "    \n",
    "    for i in range(len(sentences) - 2):\n",
    "        word = sentences[i].val\n",
    "        word_i = sentences[i + 1].val\n",
    "        word_ii = sentences[i + 2].val\n",
    "        ls.append(f'{word}_{word_i}_{word_ii}')\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_gram_dict(data):\n",
    "    unigram_dict = {}\n",
    "    bigram_dict = {}\n",
    "    trigram_dict = {}\n",
    "    N = 0\n",
    "    V = 0\n",
    "    \n",
    "    num_sentences = 0\n",
    "    for sentences in split_sentences(data):\n",
    "        num_sentences += 1\n",
    "        \n",
    "        for n_gram in unigram(sentences):\n",
    "            N += 1\n",
    "            if n_gram in unigram_dict:\n",
    "                unigram_dict[n_gram] += 1\n",
    "            else:\n",
    "                unigram_dict[n_gram] = 1\n",
    "                V += 1\n",
    "        \n",
    "        for n_gram in bigram(sentences):\n",
    "            if n_gram in bigram_dict:\n",
    "                bigram_dict[n_gram] += 1\n",
    "            else:\n",
    "                bigram_dict[n_gram] = 1\n",
    "                \n",
    "        for n_gram in trigram(sentences):\n",
    "            if n_gram in trigram_dict:\n",
    "                trigram_dict[n_gram] += 1\n",
    "            else:\n",
    "                trigram_dict[n_gram] = 1\n",
    "    \n",
    "    unigram_dict['</s>'] = num_sentences\n",
    "    return N, V, unigram_dict, bigram_dict, trigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_unigram(n_gram):\n",
    "    if n_gram in unigram_dict:\n",
    "        return unigram_dict[n_gram]/N\n",
    "    \n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_bigram(target, context, alpha=0, V=0):\n",
    "    part = context + '_' + target\n",
    "    \n",
    "    numerator = bigram_dict[part] if part in bigram_dict else 0\n",
    "    numerator += alpha\n",
    "    \n",
    "    denominator = unigram_dict[target] if target in unigram_dict else 0\n",
    "    denominator += V*alpha\n",
    "    \n",
    "    return 0 if denominator==0 else numerator/denominator    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_trigram(target, context, alpha=0, V=0):\n",
    "    part = context + '_' + target\n",
    "    \n",
    "    numerator = trigram_dict[part] if part in trigram_dict else 0\n",
    "    numerator += alpha\n",
    "    \n",
    "    denominator = unigram_dict[target] if target in unigram_dict else 0\n",
    "    denominator += V*alpha\n",
    "    \n",
    "\n",
    "    return 0 if denominator==0 else numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_bigram_sentences(sentences):\n",
    "#     Split sentences in to target and context.\n",
    "#     Multi all part and return value\n",
    "    \n",
    "    sentences = f'<s> {sentences} </s>' #padding\n",
    "    list_word = sentences.split()\n",
    "    len_sentences = list_word.__len__()\n",
    "    p_bigram = 1\n",
    "    \n",
    "    for i in range(1, len_sentences):\n",
    "        target = list_word[i]\n",
    "        context = list_word[i - 1]\n",
    "        p_bigram *= P_bigram(target, context)\n",
    "\n",
    "    return p_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_trigram_sentences(sentences):\n",
    "#     Split sentences in to target and context.\n",
    "#     Multi all part and return value\n",
    "    \n",
    "    sentences = f'<s> <s> {sentences} </s> </s>'  #padding\n",
    "    list_word = sentences.split()\n",
    "    len_sentences = list_word.__len__()\n",
    "    \n",
    "    p_trigram = 1\n",
    "\n",
    "    for i in range(2, len_sentences):\n",
    "        target = list_word[i]\n",
    "        context = list_word[i - 2] + '_' + list_word[i - 1]\n",
    "\n",
    "\n",
    "        p_trigram *= P_trigram(target, context)\n",
    "        \n",
    "    return p_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(sentences): \n",
    "#     Eval model by unseen text for both\n",
    "#     bigram and trigram. The less perplexity is,\n",
    "#     the better model becomes.\n",
    "    list_word = sentences.split()\n",
    "    len_sentences = list_word.__len__()\n",
    "    \n",
    "    #smooth\n",
    "    alpha = 1\n",
    "\n",
    "    #     for bigram\n",
    "    per_bigram = 1\n",
    "\n",
    "    for i in range(1, len_sentences):\n",
    "        target = list_word[i]\n",
    "        context = list_word[i - 1]\n",
    "\n",
    "        per_bigram *= 1/(P_bigram(target, context,  alpha, V))\n",
    "\n",
    "    per_bigram = pow(per_bigram, len_sentences)\n",
    "\n",
    "    # for trigram\n",
    "    per_trigram = 1\n",
    "\n",
    "    for i in range(2, len_sentences):\n",
    "        target = list_word[i]\n",
    "        context = list_word[i - 2] + '_' + list_word[i - 1]\n",
    "\n",
    "        per_trigram *= 1/(P_trigram(target, context, alpha, V))\n",
    "\n",
    "    per_trigram = pow(per_trigram, len_sentences)\n",
    "\n",
    "    return per_bigram, per_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_gram(text):\n",
    "    # TODO preprocess text\n",
    "    # \n",
    "    max_p_bigram = 0\n",
    "    word_bigram = 'none'\n",
    "    \n",
    "    max_p_trigram = 0\n",
    "    word_trigram = 'none'\n",
    "    \n",
    "    \n",
    "    for word in unigram_dict.keys():\n",
    "        p_bigram = P_bigram_sentences(text + ' ' + word)\n",
    "        p_trigram = P_trigram_sentences(text + ' ' + word)\n",
    "        \n",
    "        if p_bigram > max_p_bigram:\n",
    "            max_p_bigram = p_bigram \n",
    "            word_bigram = word\n",
    "            \n",
    "            \n",
    "        if p_trigram > max_p_trigram:\n",
    "            max_p_trigram = p_trigram \n",
    "            word_trigram = word\n",
    "            \n",
    "        \n",
    "    return f'bigram: {word_bigram}, trigram: {word_trigram}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d39af593786e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spam.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\lian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lian\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2145\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_name):\n",
    "    with open(file_name) as f:\n",
    "        # TODO: preprocess f.read()\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict = {}\n",
    "bigram_dict = {}\n",
    "trigram_dict = {}\n",
    "FILE_NAME = 'data_sample.txt'\n",
    "N = 0\n",
    "V = 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     text_test = 'toi la hoc sinh'\n",
    "    \n",
    "    data = load(FILE_NAME)\n",
    "    N, V, unigram_dict, bigram_dict, trigram_dict = make_n_gram_dict(data)\n",
    "#     print(P_bigram_sentences())\n",
    "#     print(P_trigram_sentences())\n",
    "#     perplexity()\n",
    "#     next_gram('toi la hoc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigram: sinh, trigram: sinh'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
